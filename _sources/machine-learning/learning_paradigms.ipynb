{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Learning paradigms\n",
    "\n",
    "The **no free lunch theorem** states that there is no single model that works optimally for every problem.\n",
    "We therefore need to develop a range of techniques that are well-adapted to different types of problem and data.\n",
    "\n",
    "As a result, there are many different techniques that fall under the ***machine learning*** umbrella.\n",
    "There are also multiple dimensions across which we can segment them. Understanding these paradigms - and which where each technique falls along each dimension - can help to intuit much about a technique before we have delved into its specifics.\n",
    "\n",
    "Below are some of the most important dimensions along which to categorise different techniques.\n",
    "\n",
    "## Supervised Vs Unsupervised (Vs Reinforcement) learning\n",
    "\n",
    "\n",
    "## Parametric Vs Non-parametric learning\n",
    "\n",
    "```{important}\n",
    "\n",
    "Parametric models have a fixed number of parameters.\n",
    "\n",
    "The number of parameters in a non-parametric model varies with the size of the training data.\n",
    "\n",
    "```\n",
    "\n",
    "**Parametric models** are able to hold the number of parameters constant by making strong(er) assumptions about the distribution of the data.\n",
    "The number of parameters is determined by the assumptions that we make about the model / data distribution.\n",
    "This notion of using our understanding of the problem to tightly constrain the functional form seems to be key.\n",
    "\n",
    "eg. linear regression, logistic regression\n",
    "\n",
    "- As a result of making strong assumptions about the data distribution, the models are restricted in the functional form they're able to learn.\n",
    "- When the assumptions are appropriate, this can help us to learn a model that generalises well with relatively little data.\n",
    "- This is especially true in regions of the input space that were sparsely covered by the training data.\n",
    "- However, if the assumptions are not appropriate then this all goes out of the window - so the use of parametric methods places a stronger burden on the practitioner to understand the domain before they model it.\n",
    "\n",
    "- A smaller number of parameters can also make the model faster to run at inference time and massively reduces the storage footprint of the model.\n",
    "\n",
    "In contrast, **non-parametric techniques** do not use rigid assumptions about the data distribution to constrain the functional form as tightly.\n",
    "The number of parameters is driven by the algorithm / data rather than our assumptions about the model.\n",
    "The notion of letting the data tell us what functional form & parameterisation we should take seems to be at the essence of the non-parametric school of thought.\n",
    "\n",
    "eg. K-nearest neighbours, CART / random forests, support-vector machines\n",
    "\n",
    "- Due to the absence of assumptions on the data distribution, non-parametric methods can have a greater capacity to learn a wider range of functional forms.\n",
    "- As a result of the greater flexibility, they often require more data to achieve high performance\n",
    "- But given the data, their expressiveness often facilitates greater performance\n",
    "- The intense focus on the data also makes them more susceptible to overfitting to the data\n",
    "- In many cases, the time complexity of inference and the storage footprint scales poorly"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}