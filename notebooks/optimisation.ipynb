{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%m\n"
    }
   },
   "source": [
    "# Optimisation\n",
    "\n",
    "> Throughout the notes below, we are investigating techniques for iterative, non-linear optimisations.\n",
    "> These optimisations are trying to find parameter estimations for the max likelihood, or MAP\n",
    "\n",
    "## Intro\n",
    "\n",
    "### The problem statement\n",
    "\n",
    "- Continuous nonlinear optimisations seek to find the values of a set of params $$ \\hat{\\theta} $$ that minimise a the value of an objective / cost function $$ f[\\cdot] $$\n",
    "\n",
    "$$ \\hat{\\theta} = \\underset{\\theta}{argmin}[ f[\\cdot] ] $$\n",
    "\n",
    "- Most optimisation techniques are described in terms of minimising the value of this function\n",
    "- However, in the context of ML, we are more frequently concerned with finding the parameter values that maximise the log probability\n",
    "- To turn a problem from one to the other, we simply multiply the function by -1 (ie. we now seek to minimise the negative log prob)\n",
    "\n",
    "### Convexity\n",
    "\n",
    "> In practice, this property will hold for very few problems that we'll come across\n",
    "\n",
    "- *Iterative* optimisation techniques are (typically) **local**: the next update will be determined based purely on the properties of the function at its current position (ie. local search around current position on a curve)\n",
    "- These techniques can therefore only guarantee finding a local minimia, not a global minima\n",
    "- One method for mitigating this shortcoming is to initialise the optimisation from multiple places and select the best outpu\n",
    "- **IFF** a function is convex, there is only a single minima & so the only local minimum is also the global minimum\n",
    "\n",
    "#### What is convexity?\n",
    "\n",
    "- Intuitively, a function is convex if any cord (line drawn between two points) does not intersect with any other point(s) on the function\n",
    "- This property can be established algebraically by creating a matrix of 2nd derivatives at each point on the function: if all the second derivs are +ve definite, then the function is convex\n",
    "\n",
    "> Put an illlustration here\n",
    "\n",
    "- When working with higher dimensions, we use the [Hessian matrix](hessian-matrix): if the matrix is +ve definite everywhere, then the cost function is convex and a global minima can be found\n",
    "\n",
    "## Overview\n",
    "\n",
    "- The cost function is a hyperplane through the problem space, with a dimension for each parameter in the cost function $$ f $$\n",
    "- 2 attributes need to be considered when deciding how best to update the parameter values with each iteration:\n",
    "    - the **Direction** of the update: $$ S $$\n",
    "    - the **Distance/Magnitude** of the update: $$ \\lambda $$. This is also known as the *line search*\n",
    "\n",
    "We want $$ \\lambda $$ such that:\n",
    "\n",
    "$$ \\hat{\\lambda} = \\underset{\\lambda}{argmin} [ f[\\theta^{[t]} + \\lambda s] ] $$\n",
    "\n",
    "> ie. the distance that minimises the cost\n",
    "\n",
    "where the update each iteration will then be:\n",
    "\n",
    "$$ \\theta^{[t+1]} = \\theta^{t} + \\hat{\\lambda}s $$\n",
    "\n",
    "\n",
    "## Direction of search\n",
    "\n",
    "There are 2 general methods for selecting the search direction:\n",
    "1. Steepest descent\n",
    "2. Gauss-Newton method (a special case of the Newton method)\n",
    "\n",
    "Both approaches rely on computing derivatives of the cost function wrt params @ the current position\n",
    "These approaches rely on an assumption of smooth functions so that derivatives are **well-behaved**\n",
    "\n",
    "### Calculation of derivatives\n",
    "\n",
    "1. A closed-form expression where possible\n",
    "2. If not, estimates can be made using finite differences:\n",
    "\n",
    "The 1st derivative of $$ f[\\cdot] $$ wrt the jth element of $$ \\theta $$:\n",
    "\n",
    "$$  $$\n",
    "\n",
    "## Distance of search\n",
    "\n",
    "## Reparameterisation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}