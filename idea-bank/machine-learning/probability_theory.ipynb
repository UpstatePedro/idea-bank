{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probability theory\n",
    "\n",
    "## Uncertainty\n",
    "\n",
    "Probability is used to quantify our uncertainty concerning unknown events or phenomena.\n",
    "\n",
    "Uncertainty comes in 2 different flavours:\n",
    "1. **Epistemic** (also, model uncertainty): originates due to our imperfect understanding of the processes that we are modelling. Colleting more, and better, data can help to reduce this.\n",
    "2. **Aleatoric**: originates in the intrinsic variability of these processes. Collecting more data won't change this.\n",
    "\n",
    "ie. we can become extremely confident that our model correctly understands a process, but if it tells us that there's an 80% chance of outcome A and 20% chance of outcome B, then there is still uncertainty in any prediction that we make.\n",
    "\n",
    "## Fundamental concepts\n",
    "\n",
    "### Basic notation\n",
    "\n",
    "$Pr(A)$ represents the probability of an event $A$ occurring\n",
    "\n",
    "$Pr(\\bar{A})$ represents the probability that an event $A$ will not occur, and is equal to $1 - Pr(A)$\n",
    "\n",
    "### Joint probability\n",
    "\n",
    "If we need to consider the possible occurrence of more than one event, for example events $A$ and $B$, then we're interested in their **joint probability distribution**.\n",
    "\n",
    "The probability that they both occur (ie. the joint probability of A & B):\n",
    "\n",
    "$Pr(A \\wedge b) = Pr(A, B)$\n",
    "\n",
    "Under conditions where $A$ & $B$ are **independent**, then $Pr(A, B) = Pr(A)Pr(B)$\n",
    "\n",
    "### Union\n",
    "\n",
    "The Union of events $A$&$B$ is the probability that $A$ or $B$ occur, but not both:\n",
    "- We'll accept the ocurrence of either $A$ or $B$ to satisfy the criteria, so we can add their probabilities together.\n",
    "- But we won't accept their joint occurrence, so we need to subtract the probability that **both** $A$ and $B$ occur together from the total\n",
    "\n",
    "$Pr(A \\vee B) = Pr(A) + Pr(B) - Pr(A \\wedge B)$\n",
    "\n",
    "> If the two events are mutually exclusive (they can't both occur at the same time) then $Pr(A \\wedge B)$ is $0$ and can be ignored.\n",
    "\n",
    "### Conditional probability\n",
    "\n",
    "When we're working with multiple interdependent events, information about the status of one event can tell us something about the status of other over events. We can therefore update our beliefs about these events in light of this information.\n",
    "\n",
    "If we know the joint probability of $A$ & $B$, this is the probability of $A$ **and** $B$ occurring together and is calculated by multiplying the two distributions together.\n",
    "If we also know the  probability distribution of event $A$, then dividing the joint probability by $Pr(A)$ must yield $Pr(B)$.\n",
    "\n",
    "More formally; the **conditional probability** of event $B$ occurring, given that event $A$ occurs, is given as follows:\n",
    "\n",
    "$Pr(B|A) \\triangleq \\frac{Pr(A,B)}{Pr(A)}$\n",
    "\n",
    "For example,\n",
    "- Event $A$ represents the probability that it rains each morning\n",
    "- Event $B$ represents the probability of there being a traffic jam outside my house in the morning\n",
    "- After months of measurements, we believe that the probability of it raining **AND** there being a traffic jam outside my house $Pr(A, B) = 0.5$\n",
    "- If we also know that the probability of it raining on any given morning is $Pr(A) = 0.7$, then we can use conditional probability to estimate the probability of a traffic jam specifically on the mornings that it rains:\n",
    "\n",
    "$Pr(B|A) = \\frac{Pr(A, B)}{Pr(A)} = \\frac{0.49}{0.7} = 0.7$\n",
    "\n",
    "So whilst we believe that the probability of having rain & traffic jams together on any given morning is approx. 50%, as soon as we know that its raining outside, we can update our expectation that there will be a traffic jam to 70%.\n",
    "\n",
    "> NB. If $Pr(A) = 0$, its occurrence is not possible and $Pr(B|A)$ would be undefined.\n",
    "\n",
    "### Independence & conditional independence\n",
    "\n",
    "If two events are **independent**, then the outcome of one event has no impact on our beliefs about the outcome of the other; our belief about one event does not *depend* on our beliefs about the other.\n",
    "In this case, their joint probability is simply the product of their individual probabilities:\n",
    "\n",
    "$Pr(A, B) = Pr(A)Pr(B)$\n",
    "\n",
    "If the probability of either event is independent of the other, then the conditional probability of either event given the other will collapse into the probability distribution of the single event. We call this **conditional independence**:\n",
    "\n",
    "$Pr(A|B) = Pr(A)$\n",
    "\n",
    "In scenarios where an event depends on several others, information on one of these dependent events (an 'intermediate' event) can contain all the information we need concerning (some of) the others. In this case, conditioning on the intermediate event allows us to treat the main event as independent of the others.\n",
    "\n",
    "#### An example\n",
    "\n",
    "- Let $A$ represent the event 'the bus will arrive in the next 5 minutes'.\n",
    "- $B$ is the event 'the bus can be seen leaving the previous stop'. (once we can see the bus leaving the previous stop, it never takes more than 5 minutes to get to our stop)\n",
    "- And $C$ is the event 'the screen at the bus stop says the bus will arrive in the next 5 minutes'.\n",
    "\n",
    "Clearly $A$ is dependent on $C$: if we can't see the bus but the sign says it will be here in the next 5 minutes, then this influences our beliefs about $A$.\n",
    "However, as soon as we can see the bus leaving the previous stop then looking at the bus stop sign will no longer influence our beliefs about $A$; $A$ becomes **conditionally independent** of $C$, given $B$.\n",
    "\n",
    "$Pr(A|B,C) = Pr(A|B)$\n",
    "Or, equivalently:\n",
    "$Pr(A,C|B) = Pr(A|B)Pr(C|B)$\n",
    "\n",
    "This can be written $A \\perp C|B$\n",
    "\n",
    "### Random variables\n",
    "\n",
    "Let $X$ represent a variable; a quantity whose value we are interested in.\n",
    "If we do not know the value of $X$ with certainty (ie. there is uncertainty concerning the current value, or how the value will change with time), then we call $X$ a **random variable**.\n",
    "\n",
    "> The **state space** $\\mathcal{X}$: the set of possible values that $X$ can take.\n",
    "\n",
    "### Discrete random variables\n",
    "\n",
    "**Discrete**: the number of possible values that a random variable can take is restricted to a finite set\n",
    "\n",
    "Examples:\n",
    "\n",
    "- Binary classes: 1, 0\n",
    "- Categories: 'small', 'medium', 'large'\n",
    "- Specific set of values: {0, 1, 2, 3, 4, 5}\n",
    "\n",
    "#### Probability mass function (pmf)\n",
    "\n",
    "The probability that $X$ takes a specific value $x$ is denoted $Pr(X=x)$\n",
    "\n",
    "We can treat scenarios where $X$ takes one of its possible values $x$ as an event.\n",
    "\n",
    "The **probability mass function** is a function which computes the probability of these events for each/any possible value of a **discrete** random variable.\n",
    "\n",
    "$p(x) \\triangleq Pr(X=x)$\n",
    "\n",
    "The **pmf** satisfies 2 properties:\n",
    "1. Always returns a value between 0 & 1 (inclusive): $0 \\leq p(x) \\leq 1$\n",
    "2. The sum of $p(x)$ across all values in the state space always equals 1: $\\sum_{x \\in \\mathcal{X}} p(x) = 1$\n",
    "\n",
    "### Continuous random variables\n",
    "\n",
    "**Continuous**: the random variable can take on any of the infinite possible values that sit on the range between two real values (possibly infinite)\n",
    "\n",
    "#### Cumulative distribution function (cdf)\n",
    "\n",
    "When working with continuous random variables, it is no longer possible to create a countable set of distinct possible values that the variable can take. The **pmf** therefore does not apply to continuous variables.\n",
    "\n",
    "Instead, we can partition the real line into a countable set of intervals and we can define events where $X$ takes any value within defined intervals.\n",
    "\n",
    "The **cumulative distribution function** is a function which computes the probability that $X$ takes a value equal to, or less than, some possible value $x$:\n",
    "\n",
    "$P(x) = Pr(X \\leq x)$\n",
    "\n",
    "The **cdf** satisfies the following properties:\n",
    "1. Always returns a value between 0 & 1 (inclusive): $0 \\leq P(x) \\leq 1$\n",
    "2. $P(x)$ across the full interval of possible values equals 1\n",
    "3. $P(x)$ is a **monotonically non-decreasing function**\n",
    "\n",
    "From the **cdf**, it is possible to calculate the probability that $X$ lies inside any interval:\n",
    "$Pr(a < X \\leq b) = P(b) - P(a)$\n",
    "\n",
    "#### Probability density function (pdf)\n",
    "\n",
    "The **pdf** is the derivative of the **cdf**\n",
    "\n",
    "$p(x) \\triangleq \\frac{d}{dx}P(x)$\n",
    "\n",
    "#### Percent point function (ppf)\n",
    "\n",
    "Also known as the ***inverse cdf*** or ***quantile function***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laws\n",
    "\n",
    "### Sum rule\n",
    "\n",
    "Also know as the ****rule of total probability***\n",
    "\n",
    "### Product rule\n",
    "\n",
    "\n",
    "### Chain rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes' rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Properties of a distribution\n",
    "\n",
    "### Moments\n",
    "\n",
    "#### Mean\n",
    "\n",
    "bla\n",
    "\n",
    "#### Variance\n",
    "\n",
    "\n",
    "#### Mode\n",
    "\n",
    "\n",
    "### Conditional moments\n",
    "\n",
    "#### Covariance\n",
    "\n",
    "#### Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Central limit theorem"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Idea-Bank (Py3.7)",
   "language": "python",
   "name": "idea-bank"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
